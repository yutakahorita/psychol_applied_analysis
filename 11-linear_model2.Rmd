```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(knitr)
set.seed(1)
```


# 線形モデル（t検定, 分散分析, 共分散分析）{#chap11_lmmodels}

前の章では、線形モデルの概要を見てきた。前の章で線形モデルを用いて行った分析は、要は「単回帰分析（一つの連続変量から、もう一方の連続変量の値を予測する分析）」であった。  
  
この章では、線形モデルを用いて他の分析を行う。予測変数のパターンが異なるケースの演習を通して、線形モデルは正規分布を扱うあらゆるタイプの分析を内包したモデルであることを確認していく。  

* 変数の標準化  
* 予測変数がカテゴリの場合（ダミー変数）  
* 予測変数がカテゴリの場合（複数のダミー変数）  
* 交互作用    


## 準備{#chap11_preparation}
  
必要なパッケージをロードする。

```{r, eval=FALSE}
library(dplyr)
library(ggplot2)
```

  
## 線形モデルに含まれる統計解析{#chap11_lmgroup}  
  
線形モデルとは特定の解析を指すものではなく、正規分布を扱う様々な統計解析を包括的に扱う統計モデルである。例えば、変数の正規性を前提とするt検定や分散分析も線形モデルの中に含まれる。予測変数の種類や個数の違いによって、線形モデルは以下のそれぞれの統計解析と一致する。
  
```{r, echo=FALSE}

knitr::kable(data.frame(分析 = c("t検定", "分散分析", "共分散分析", "単回帰分析", "重回帰分析"),
                          予測変数 = c("二値(0 or 1)", "二値", "二値及び連続量", "連続量", "連続量（二値を含んでも可）"),
                          予測変数の個数 = c("1個", "2個以上", "二値が2個以上、連続量が1個以上", "1個", "2個以上")))

```

前章では単回帰分析（予測変数が連続量で1個）を例として演習を行った。以下では、t検定や分散分析と似たことを`lm()`関数で行う練習を通して、**予測変数がカテゴリーの場合**の扱い方を学んでいく。


## 変数の標準化{#chap11_scaling}

解析の演習に入る前に、**標準化(standardizing)**について確認しておこう。標準化とは、元の値を「ゼロが平均値、1が標準偏差」と等しくなるように値を変換する処理のことをいう。変数を標準化しておくと、線形モデルの係数の解釈が直感的に理解しやすくなる事が多い。  
  
例えば、前の章では`iris`データを使って単回帰分析を行った。

```{r}

dat = iris #ここではirisを別の名前（dat）に変えて練習に用いる
result = lm(data = dat, Petal.Length ~ 1 + Sepal.Length)
summary(result)

```

切片の値は`Sepal.Length`がゼロのときの`Petal.Length`の予測値である。しかし、アヤメの花弁の長さがマイナスやゼロの値を取るというのはありえない。また、切片は予測変数がゼロのときの応答変数の値であるが、このデータも予測変数（すなわちがくの長さ）がゼロの場合も論理的には有り得ない。このように、元の値をそのまま使っても係数の解釈にこまる場面がある。  
  
  
応答変数及び予測変数を標準化したものを使って同じ解析をして、結果を比較してみよう。具体的には、元の得点から**平均値を引いて**差の得点を求め、その差の得点を**標準偏差で割る**。


```{r}

dat = dat |> dplyr::mutate(Petal.Length_std = (Petal.Length - mean(Petal.Length))/sd(Petal.Length),
                           Sepal.Length_std = (Sepal.Length - mean(Sepal.Length))/sd(Sepal.Length))

head(dat)

```

標準化した得点を用いて線形モデルで解析を行う。

```{r}

result_std = lm(data = dat, Petal.Length_std ~ 1 + Sepal.Length_std)
summary(result_std)

```


標準化しない得点を使ったときの解析結果と比べると、係数の値が変わっている。しかし、傾きのt値及びp値は変わっていない（標準化は単にデータを平行移動させただけなので、回帰直線の傾きは変わらない）。  
  
切片は`r as.numeric(round(result_std$coefficients[1],2))`、`Sepal.Length_std`の効果は`r as.numeric(round(result_std$coefficients[2],2))`である。切片の値は、`Sepal.Length_std`がゼロのとき、つまり`Sepal.Length`が平均値と等しいとき、`Petal.Length_std`はほぼゼロの値を取る、つまり`Petal.Length`の平均値であることを意味している。また、`Sepal.Length_std`の傾きは、`Sepal.Length_std`が1のとき（つまり`Sepal.Length`が1標準偏差分増加したとき）、`Petal.Length_std`が`r as.numeric(round(result_std$coefficients[2],2))`増えることを意味する。  
  
このように、標準化することで算出された係数の解釈がしやすくなることが多い。以降の線形モデルを扱う分析の例でも、応答変数は標準化したものを用いる。

***
Rには標準化を行うための関数も用意されている。`scale()`を用いる。

```{r}

dat = iris |> dplyr::mutate(Petal.Length_std = scale(Petal.Length)) 

head(dat)

```


## 予測変数がカテゴリカル変数の場合（ダミー変数）{#chap11_dummy}

予測変数はカテゴリカル変数（質的変数）でも構わない。ただし、予測変数を0か1のどちらかの値を取る**ダミー変数**(dummy variable)に変換する必要がある。  
  
Rに入っている`sleep`データを少し変えたもの使って、カテゴリカル変数を予測変数に含む線形モデルの解析をしてみよう。

```{r}
sleep_1 = sleep |> dplyr::select(-ID) |> 
  dplyr::mutate(group = ifelse(group == 1, "control", "treatment"))
sleep_1
```

`group`はグループを意味する変数である（統制群`control`もしくは実験群`treatment`）。まずこれを、「`treatment`なら1、`control`なら0」とする新たな変数`dgroup`を作る。  

```{r}
sleep_1 = sleep_1 |> dplyr::mutate(dgroup = ifelse(group == "treatment", 1, 0))
sleep_1
```

`ifelse()`関数は、`ifelse(XXX, A, B)`と表記することで、「XXXの条件に当てはまればA、当てはまらなければB」という処理をしてくれる。ここでは、変数`group`について、`treatment`ならば1, それ以外なら0に変換し、0か1を取る変数`dgroup`を新たに作った。  
  
この`dgroup`が*ダミー変数*である。  
  
解析に用いるモデルを確認すると、以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta dgroup \\ \tag{1}\\
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$

$dgroup$は0か1のどちらかを取る変数で、$dgroup = 0$のとき、つまり統制群のとき、応答変数の予測値は$\mu = \alpha$となる。$dgroup = 1$のとき、つまり実験群のとき、応答変数の予測値は$\mu = \alpha + \beta$となる。すなわち、切片$\alpha$は統制群のときの効果、傾き$\beta$は実験群の時に加わる実験群特有の効果を意味する。  

`lm()`を使って、上のモデル式のパラメータの推定をしよう。

```{r}
sleep_1 = sleep_1 |> dplyr::mutate(extra_std = scale(extra)) #応答変数を標準化したものを用いる

result = lm(data = sleep_1, extra_std ~ 1 + dgroup)
summary(result)

```

2つの群間で平均値を比較するときにはt検定がよく使われる。`t.test()`関数を使って$dgroup=0$と$dgroup=1$との間で$y$の値の平均値を比較したときのt値及びp値の結果が、`lm()`の傾きのt値及びp値と一致することを確認しよう。

```{r}

t.test(data = sleep_1, extra_std ~ dgroup)

```

`lm()`の傾きの検定は、「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは、予測変数$dgroup$が1単位増えたときの応答変数$y$の変化量であった。傾きの検定は、「$dgroup=0$ から $dgroup=1$ に変化することによって、 応答変数（`extra`） が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は、「$dgroup=0$と$dgroup=1$の間で$y$の値に差があるか」を検定しているのと論理的に同じである。  
  
このように、*予測変数が1つで、予測変数が二値（0もしくは1）であるときの線形モデルは、t検定に対応する*。  


## グループが複数ある場合（一要因の分散分析）{#chap11_onewayANOVA}

先ほどの例は、統制群と実験群の二つのグループの場合であった。例えば実験で統制群、実験群1、実験群2といったように三つ以上のグループを設定した場合は、どうダミー変数を作成すればよいのか？  
  
Rに入っている`PlantGrowth`を例として見ていこう。以下のプログラムを実行して、データを作ろう。


```{r}
dat = PlantGrowth |>
  dplyr::mutate(t1 = ifelse(group == "trt1", 1, 0),
                t2 = ifelse(group == "trt2", 1, 0)
                )
dat
```

ダミー変数を2つ作成した。`ctrl`のときは「`t1` = 0, `t2` = 0」,`trt1`のときは「`t1` = 1, `t2` = 0」,`trt2`のときは「`t1` = 0, `t2` = 1」となっている。これら２つのダミー変数を線形予測子に加えたモデルは、モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{t1} t_1 + \beta_{t2} t_2  \\ \tag{3}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$
切片$\alpha$は「`t1` = 0, `t2` = 0（すなわち、`ctrl`のとき）」の$\hat{y}$の値と等しい。  
傾き$\beta_{t1}$は「`t1` = 1, `t2` = 0（すなわち、`trt1`のとき）」の$\hat{y}$の値と等しい。  
傾き$\beta_{t2}$は「`t1` = 0, `t2` = 1（すなわち、`trt2`のとき）」の$\hat{y}$の値と等しい。  
  
このように、グループの数が$K$個ある場合には、ダミー変数を$K-1$個作れば全てのグループの予測値$\hat{y}$を線形予測子で表すことができる。

`lm()`で傾き及び切片のパラメータを推定しよう。

```{r}

dat = dat |> dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる
result = lm(data = dat, weight_std ~ t1 + t2 + 1) 
summary(result)

```


式（3）より、切片の推定値は$t_1=0$かつ$t_2=0$のときの$\mu$、つまり統制群(ctrl)のときの応答変数`weight_std`の推定値を意味している。各ダミー変数の係数（傾き）は、切片に加わる各条件の効果を意味している。例えば、`t2`の係数は`r round(as.numeric(result$coefficients[3]),2)`であるが、これは$t_2$のとき（つまりtrt2のとき）の応答変数の予測値は、 `r round(as.numeric(result$coefficients[1]) + as.numeric(result$coefficients[3]),2)`(= 切片 + `t2`の傾き)となることを示している。  
  
係数の意味することは、基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。  
  

図でも条件別に`weight_std`の分布を確認してみよう。分布を見ても同様の傾向があるが、線形モデルの解析の結果その効果が有意であることが確認できた。
  
```{r}

ggplot() + 
  geom_boxplot(data = dat, aes(x = group, y = weight_std))

```


モデル（式）を確認しながら、係数が何を意味しているのかを常に意識するようにしよう。

***

このテキストでは練習のためにダミー変数を自分でプログラムを書いて作っているが、`lm()`関数にカテゴリカル変数をそのまま入れても結果を出力してくれる。カテゴリのうちアルファベット順で最初に出てくるカテゴリ(以下の`PlantGrowth`の例では`ctrl`)を基準として、残りのダミー変数を自動で作ってくれている。

```{r}

dat = PlantGrowth |> dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる
result = lm(data = dat, weight_std ~ group) 
summary(result)

```



## 交互作用（2要因の分散分析）{#chap11_interaction}

次は、線形モデルで**交互作用**を扱う方法について確認する。2要因以上の分散分析と同様のことを線形モデルで行う。  
  
以下のプログラムを実行して、サンプルデータ`d`を作ろう。


```{r}

set.seed(1)
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")

x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

d = rbind(d_M, d_F)

head(d)

```


このデータ`d`には、`x`, `y`, `gender`の3つの変数が含まれている。`gender`は性別を意味する変数とする。M（男性）かF（女性）のいずれかである。男女別に、実験で2つの変数を測定したとしよう。

応答変数を`y`、予測変数を`x`として線形モデルで切片及び`x`の傾きのパラメータを推定する。モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta x  \\ \tag{4}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$


`lm()`関数を使って推定しよう（$x$と$y$の散布図及び係数の信頼区間も図示する）。

```{r}
result = lm(data = d, y ~ 1 + x)
summary(result)

newdat = data.frame(x = seq(1,10,0.1))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(x = seq(1,10,0.1), result_conf)

ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y), size = 3) + 
  ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit)) + 
  ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 
  
```

予測変数`x`の傾きはほぼフラットで、`y`に対してあまり効果がないようにみえる。  
  
しかし、このデータ`d`にはもう一つ性別を意味する`gender`という変数が含まれていた。`gender`を区別して、また`x`と`y`の散布図を見てみよう。

```{r}
ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 

```

性別が女性（F）か男性（M）かで、`x`と`y`の関係が違うようである。  
このように、別の変数との組み合わせにより、変数間の関係が変化することを**交互作用(interaction)**という。このデータでも、応答変数`y`に対して性別`gender`と`x`の交互作用がありそうである。  
  
交互作用のあるモデルは、以下のように表現する。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x + \beta_{2} M + \beta_{3} xM  \\ \tag{5}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$

$M$は性別`gender`のダミー変数で、`M`（男性）ならば1、`F`（女性）ならば0の変数とする。  
線形モデルでは、**交互作用は予測変数同士の積**で扱う。式5の線形予測子を$x$で整理すると、

$$
\begin{equation}
  \hat{y} = \alpha +(\beta_{1} + \beta_{3}M) x +\beta_{2}M  \\ \tag{6}
\end{equation}
$$
となる。つまり、男性の場合には式6に$M=1$を代入すると、

$$
\begin{equation}
  \hat{y} = \alpha +(\beta_{1} + \beta_{3}) x +\beta_{2}
\end{equation}
$$

となるので、男性の時の$x$の効果は、$x$に係る傾き$\beta_{1}$と$x$と$M$の積に係る傾き$\beta_{3}$をあわせたものになる。  
  
一方、女性の場合には、式6にM=0を代入すると

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x  \\ 
\end{equation}
$$

となる。つまり、$x$に係る傾きである$\beta_{1}$が女性の時の$x$の効果を意味する。  
  
言い換えると、交互作用項にかかる傾き$\beta_{3}$は、男性のときに女性の$x$の効果に追加分でかかる$x$の傾きの変化量を意味することになる。
  
このように、交互作用を考慮する予測変数の積をモデルに加えることで、男性か女性かで切片及び傾きが変化することを表現できる。

```{r}

d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る
result = lm(data = d, y ~ 1 + x*M)
summary(result)

```

2つの予測変数の積の傾き（$\beta_{3}$）は、`x:M`である。p値も小さく、有意な効果を持っているようである。  
  
サンプルデータについて、推定されたパラメータを元に、男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。  
  
```{r}

new_x = seq(1,10,0.1)
newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x))))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(newdat, result_conf)
plot_conf$gender = ifelse(plot_conf$M == 1, "M", "F")

ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + 
  ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + 
  ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 

```

### 交互作用を扱うモデルの解釈{#chap11_NotingInteraction}  
  
2要因以上の分散分析と同様に、線形モデルでも交互作用の有無を検討することができる。しかし、ここで注意が必要なのは、交互作用を含む線形モデルは解釈が複雑になることである。  
分散分析では、変数の組み合わせの効果である交互作用とは別に、その変数そのものの効果である主効果についての検定も行われる。では、上述の線形モデルの$\beta_{1}$及び$\beta_{2}$のパラメータはそれぞれ、性別$M$及び予測変数$x$の主効果として解釈できるのだろうか？  
  
もう一度、式5の線形予測子を見てみよう。

$$
\hat{y} = \alpha + \beta_{1} x + \beta_{2} M + \beta_{3} xM  \\ \tag{5}
$$

$M = 0$を代入すると、$\hat{y} = \alpha +\beta_{1} x$となる。$M = 0$はつまり女性であることを意味するので、$\beta_{1}$は「データが女性であるときの、予測変数$x$が１単位増加したときの応答変数$y$の変化量」を意味する。
つまり、交互作用項を含む際のある予測変数の傾きは、**他の予測変数がゼロであるときの予測変数の効果**を意味している。すなわち、$x$の傾き$\beta_{1}$は「限定的な条件のもとでの予測変数の応答変数に対する効果」を示しており、主効果の一般的理解である「応答変数に対する予測変数の平均的効果」とは異なる。  

このように、交互作用を含むモデルの場合、**予測変数の傾きは必ずしも主効果を意味するわけではない**。次の節の補足で、この問題に対する対処法について説明する。


***

予測変数（ダミー変数も含む）を標準化した場合には、各予測変数の傾きの解釈の仕方も変わってくる。


```{r}

#データを再度作成する。
set.seed(1)
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")

x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

d = rbind(d_M, d_F)
d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る

```

```{r}

#標準化する前の結果
result = lm(data = d, y ~ 1 + x*M)
summary(result)

```

応答変数と予測変数を標準化する。ダミー変数も標準化する。

```{r}

#変数を標準化
d = d |> dplyr::mutate(y_std = scale(y),
                       x_std = scale(x),
                       M_std = scale(M)
                       )
head(d)

result = lm(data = d, y_std ~ 1 + x_std*M_std)
summary(result)

```

それぞれの係数とp値が変わった。  
  
変数を標準化する前のモデルでは、パラメータ$\beta_{1}$は「女性のときの`x`の傾き」であり、特定のカテゴリのときの予測変数`x`が応答変数に及ぼす影響を意味していた。  
これに対し、標準化した後のモデルでは`x_std`の傾きが意味することは、「**他の変数がゼロのとき、つまり平均であるとき**に、`x_std`が1単位（1標準偏差）変化したときの応答変数の変化量」を意味することになる。他の変数が特定の条件である場合ではなく、平均的水準であるときの予測変数の効果に関心がある場合は標準化した変数を用いるとよい。  


## 予測変数が複数ある場合（共分散分析または重回帰分析）{#chap11_ANCOVA}

予測変数は連続量もカテゴリカル変数でも何でも含めても良い。  
  
### 変数の効果の統制{#chap11_Controlling}

予測変数を複数加えた線形モデルの解析のメリットは、ある予測変数について他の予測変数の効果を**統制(control)**したときの効果を検討できることにある。  
  
Rで標準で入っている`attitude`データを使って、予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。

```{r}
head(attitude)
```

以下のように、`complaints`, `privileges`, `learning`, `raises`の4つを予測変数として、`rating`の値の推定を行ってみよう。

```{r}
dat = attitude |> dplyr::mutate(rating_std = scale(rating),
                                complaints_std = scale(complaints),
                                privileges_std = scale(privileges),
                                learning_std = scale(learning),
                                raises_std = scale(raises)
                                ) #変数を標準化したものを使う

result = lm(data = dat, rating_std ~ 1 + complaints_std + privileges_std + learning_std + raises_std)
summary(result)
```


切片（Intercept）は全ての予測変数の値がゼロ（ここでは標準化しているので平均）のときの応答変数の予測値である。  
  
各予測変数の傾きの値は「**他の変数の影響を統制したとき**の、その予測変数が1単位変化したときの応答変数量の変化量」を意味する。例えば、`complaints`の係数`r round(as.numeric(result$coefficients[2]), 2)`は、その他の予測変数`privileges`, `learning`, `raises`の値が一定だと仮定したときの、`complaints`が`rating`に与える効果を示す。言い換えれば、モデルに含まれる他の予測変数の値に関係なく、`complaints`が独立に`rating`に与える効果を意味する。  
  
複数の予測変数を入れることで、他の予測変数の影響を統制した上での予測変数が応答変数に及ぼす影響を検討することができる。   



## まとめ{#chap11_Summary}

この章では、予測変数がカテゴリカル変数の場合及び交互作用を含むモデルの場合を学んできた。

- 予測変数がカテゴリカル変数の場合は、0か1の値を取るダミー変数にして線形予測子に投入する。  
- 3つ以上のカテゴリの場合は、カテゴリの数-1個分のダミー変数を線形予測子に投入する。  
- 2つの予測変数の組み合わせの効果（交互作用）を見たい場合は、2つの予測変数の積を線形予測子に投入する。  
- 予測変数を複数加えたときの各予測変数の傾きは、他の予測変数の影響を統制した上での、その予測変数の応答変数に及ぼす効果を意味する。  

線形予測子を拡張することで、正規分布を扱う様々な統計解析を線形モデルを扱う関数(`lm`)のみで行うことができることを学んできた。  
  
この章では、応答変数は正規分布に従うという前提をおいてきたが、応答変数が従う確率分布を正規分布以外にすることも可能である。以降の章では、応答変数が従う確率分布を変更して一般化した「一般化線形モデル」を扱っていく。

  
## 確認問題 {#chap11_Practice}
  
### 問1 {-}

Rで標準で入っているデータ`warpbreaks`を使って練習をする。  


```{r}

prac_dat_1 = warpbreaks #別の名前で保存する

head(prac_dat_1)
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks))
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks))

```


#### 1-1 {-}


変数`wool`について, 「`A`を1, それ以外を0」としたダミー変数を作成し、そのダミー変数を予測変数、`breaks`を応答変数として線形モデルを行い、切片及びダミー変数に係る傾きの推定値を報告せよ。  
  
また、ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。

```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy = ifelse(prac_dat_1$wool == "A", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ 1 + dummy))

```

#### 1-2 {-}

変数`tension`について, 「`L`を1, それ以外を0」、「`M`を1, それ以外を0」とした2種類のダミー変数を作成し、それら2つのダミー変数を予測変数、`breaks`を応答変数として線形モデルを行い、切片及び各ダミー変数に係る傾きの推定値を報告せよ。    
更に、そのときの切片及び各ダミー変数の係数が意味することを説明せよ。  


```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ 1 + dummy_1 + dummy_2))

```




```{r eval=FALSE, include=FALSE}

prac_dat_1$breaks_2 = prac_dat_1$breaks - mean(prac_dat_1$breaks)

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)
prac_dat_1$dummy_3 = ifelse(prac_dat_1$tension == "H", 1, 0)

summary(lm(data = prac_dat_1, breaks_2 ~ 1 + dummy_1 + dummy_2 + dummy_3 - 1))

```

  
### 問2 {-}

問1に引き続き、Rで標準で入っているデータ`warpbreaks`を使って練習をする。ただし、`tension`が`H`の部分を除いたデータを用いる。 


```{r}

prac_dat_2 = subset(warpbreaks, tension != "H") #tension == Hは除き、別の名前で保存する

head(prac_dat_2)

```

```{r}
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension))

```


breaksを応答変数、wool, tension, wool及びtensionの交互作用項を予測変数とした線形モデルを行い、切片、woolの傾き、tensionの傾き、交互作用項の推定値を報告せよ。  

```{r eval=FALSE, include=FALSE}

summary(lm(data = prac_dat_2, breaks ~ 1 + wool * tension))

```



## 補足１(emmeans){#chap11_emmeans}

この章では、ダミー変数を複数加えたモデル及び交互作用を加えたモデルで一要因の分散分析や二要因の分散分析と同様のことができることを学んできた。ただし、分散分析では条件の効果が有意だった場合には、どの条件間で差があるかを多重比較補正をした上で比較するプロセスが必要となる。  
  
ここでは、`emmeans`パッケージを用いて`lm()`関数で多重比較補正を行う手順を紹介する（詳細は`emmeans`パッケージのヘルプを参照のこと）。

```{r, message = FALSE, warning = FALSE}
library(emmeans)
```

### 一要因の分散分析における多重比較{#chap11_emmeans1}

```{r, message = FALSE, warning = FALSE}
dat = PlantGrowth |> dplyr::mutate(weight_std = scale(weight))
result = lm(data = dat, weight_std ~ group) 

emm = emmeans::emmeans(result, ~ group) #モデルから推定された各カテゴリのmarginal mean(emmeans)を算出する
emm

emmeans::contrast(emm, method = "pairwise", adjust = "tukey")    #pairwiseで条件の組み合わせごとのemmeansの比較の結果が表示される。adjustで多重比較補正の方法を指定できる。

```


### 二要因の分散分析における単純主効果検定{#chap11_emmeans2}

```{r, message = FALSE, warning = FALSE}
dat = warpbreaks |> dplyr::mutate(breaks_std = scale(breaks))
result = lm(data = dat, breaks_std ~ wool * tension)

emm = emmeans::emmeans(result, ~ wool * tension)
emm

emmeans::contrast(emm, method = "pairwise", by = "wool")#wool内でtension間の比較を行う
emmeans::contrast(emm, method = "pairwise", by = "tension") #tension内でwool間の比較を行う

```

## 補足２（単純傾斜）{#chap11_simpleslope}

本章では２つの変数の積を加えた交互作用のモデルを扱ったが、ここで交互作用を構成する２つの変数について$X$を「予測変数」、$M$を「**調整変数**」と区別する。  
**調整変数(moderating variable)**：予測変数$X$と応答変数$Y$との関係を調整する変数  
    
予測変数$X$と調整変数$M$の積である交互作用を含むモデルでは、予測変数の傾きが意味するのは予測変数そのものの効果ではなく、**調整変数がゼロの時に予測変数が1単位変化したときの応答変数の変化量**であった。  
このような調整変数が特定の値を取るときの予測変数が応答変数に与える効果のことを**単純傾斜(simple slope)**と表現する。単純傾斜は、交互作用がいかなるかたちで生じているかの解釈に用いられる。
  

### 調整変数がカテゴリカル変数，予測変数が量的変数のとき{#chap11_ss_1}

本章で交互作用の解説で用いたサンプルデータを再び例にして、単純傾斜の推定方法について理解する。

```{r}

set.seed(1)
#男性のみのデータ
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")
#女性のみのデータ
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

#２つを合わせたデータ
d = rbind(d_M, d_F)

head(d)

```

性別を調整変数、$x$を予測変数として考える。  
本章で行った分析と同様に、男性（`gender == M`）ならば1，女性（`gender == F`）ならば0のダミー変数`M`を作成し、調整変数`M`と予測変数`x`、さらに２つの積である交互作用（`x:M`）を加えた線形モデルを実行する。


```{r}

#ダミー変数の作成
d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る

result_1 = lm(data = d, y ~ 1 + x*M) #F=0, M=1のダミー変数を使った場合
round(coef(summary(result_1)), 3) #係数を確認する

```

`x`の傾きは調整変数`M`がゼロのとき、すなわち女性の`x`の傾き（`x`が1単位増加したときの`y`の変化量）を意味していた。  

式で表すと、

$$
\hat{y} = \alpha + \beta_{1} x + \beta_{2} M + \beta_{3} xM  \\ 
$$

である。式からも、$x$の傾きである$\beta_{1}$は、調整変数$M$がゼロ（すなわち、女性）のときに$x$が1単位増えたときの$y$の変化量であることが明らかである。

  
逆に、男性の時の`x`の傾きを知りたいときにはどうすればよいか？この場合、先程のダミー変数のコーディングを逆にしたダミー変数を代わりに使えば良い。つまり、「女性ならば1, 男性ならば0」のダミー変数`F`を作成し、調整変数`F`, 予測変数`x`及び交互作用項`x:F`から`y`を予測する線形モデルを実行してみる。

```{r}
d$F = ifelse(d$gender == "F", 1, 0) #genderがFならば1, Mならば1のダミー変数を作る

result_2 = lm(data = d, y ~ 1 + x*F) #F=1, M=0のダミー変数を使った場合
round(coef(summary(result_2)), 3) #係数を確認する

```

この`x`の傾きが、調整変数`F`がゼロのとき、すなわち男性の`x`の傾きを意味する。 


式で表すと、

$$
\hat{y} = \alpha + \beta_{1} x + \beta_{2} F + \beta_{3} xF  \\ 
$$

である。  
  
このように、ダミー変数のコーディングを工夫することで、それぞれの調整変数別に予測変数と応答変数との関連（単純傾斜）を推定することができる。  
  
`interactions`パッケージの`sim_slopew`関数を使えば、ダミー変数を作り直さなくても調整変数のカテゴリごとに単純傾斜を算出することができる。

```{r}
library(interactions)
interactions::sim_slopes(result_1, pred = x, modx = M) #predに予測変数、modxに調整変数を入れる。
interactions::interact_plot(result_1, pred = x, modx = M, plot.points = TRUE) #単純傾斜をプロットすることもできる

```

また、`emmeans`パッケージの`emtrends`でも同様に推定することができる。

```{r}
emtrend_1 = emmeans::emtrends(result_1, specs = "M", var = "x") #specsに調整変数, xに予測変数を指定する。
emtrend_1
```

それぞれで推定された結果が、先ほどの線形モデルでの$x$の傾きの推定値と一致していることがわかる。  
  
また、`emtrends`では単純傾斜の比較もすることができる（男性と女性で傾きが有意に異なるか？）。この結果は、線形モデルの推定結果に表示される交互作用項の検定の結果と一致する（交互作用の傾きが意味するのは「調整変数が1であるときに、予測変数の傾きがどのくらい変化するか」であり、その増え方がゼロより有意に大きいあるいは小さいかを検定している）。

```{r}

emmeans::emtrends(result_1, pairwise ~ M, var = "x")
round(coef(summary(result_1)), 3) #係数を確認する

```

なお、上述の例は、調整変数がカテゴリカル変数で予測変数が量的変数の場合だが、両者ともカテゴリカル変数の場合でも解釈は同様である（予測変数の傾きは、調整変数がゼロのときの予測変数がゼロから1に変化したときの応答変数の変化量）。

### 予測変数と調整変数が量的変数のとき{#chap11_ss_2}

交互作用項が２つの量的変数の積である場合も、予測変数の傾きが意味することは「調整変数がゼロであるときの予測変数の傾き」という点で同じだが、モデルによっては解釈に注意が必要である。  
  
サンプルデータの解析を通して確認していく。

```{r}
#サンプルデータの作成
set.seed(1)
m = rnorm(n = 40, mean = 1, sd = 1)
x = rnorm(n = 40, mean = 2, sd = 1)
mu = 0.1 + 0.3 * x + 0.2 * m - 0.2 * x * m
y = rnorm(n = 40, mean = mu, sd = 1)
d = data.frame(x = x, m = m, y = y)

head(d)
```

`m`を調整変数、`x`を予測変数とし、応答変数`y`を予測する線形モデルを行う。


```{r}
result_3 = lm(data = d, y ~ 1 + x * m)
round(coef(summary(result_3)), 3)
```


モデルを式で表すと、以下の通りになる。

$$
\hat{y} = \alpha + \beta_{1} x + \beta_{2}  m + \beta_{3}  x m  \\ 
$$

先ほどと同様に$\beta_{1}$は$m = 0$であるときの$x$が1単位増えたときの$y$の変化量として解釈することができる。  
  
しかし、調整変数がゼロを取り得ない変数である場合には（例として身長や体重など）、係数の意味の解釈がしにくくなる（本章でも述べているように、平均値をゼロに変換する標準化も一つの対応策である）。
  
そこで、調整変数が量的変数の場合には調整変数のレベルを「平均」、「平均＋1標準偏差」および「平均 - 1標準偏差」の3つに分けて、予測変数の単純傾斜を推定することが多い。  
  
ここでは、`m`が「平均」、「平均 + 1標準偏差」および「平均 - 1標準偏差」の場合に分けて、`x`が`y`に及ぼす影響を見てみよう。

```{r}
d$m_minus_SD = d$m - (mean(d$m) - sd(d$m)) # mが平均-1標準偏差の値であるときにゼロになる変数を作成
d$m_plus_SD = d$m - (mean(d$m) + sd(d$m)) # mが平均値であるときにゼロになる変数を作成
d$m_mean = d$m - mean(d$m)  # mが平均+1標準偏差の値であるときにゼロになる変数を作成

round(coef(summary(lm(data = d, y ~ 1 + x * m_minus_SD))), 3) # mean -1SD
round(coef(summary(lm(data = d, y ~ 1 + x * m_mean))), 3) # mean
round(coef(summary(lm(data = d, y ~ 1 + x * m_plus_SD))), 3) # mean + 1SD

```

それぞれのモデルの`x`の傾きは、`m_minum_SD`, `m_mean`, `m_plus_SD`がゼロのとき、すなわち調整変数がそれぞれ平均-1標準偏差, 平均、平均-1標準偏差の値を取るときの`x`の傾きを意味する。  
  
「平均」、「平均+1標準偏差」、「平均-1標準偏差」に対応する変数を新たに作らなくても、`interactions`パッケージの`sim_slopes`でも単純傾斜を推定することができる。

```{r}
interactions::sim_slopes(result_3, pred = x, modx = m) #predに連続量の予測変数、modxに調整変数を入れる。
interactions::interact_plot(result_3, pred = x, modx = m, plot.points = TRUE) #単純傾斜をプロットすることもできる
```

```{r}
#emmeansパッケージのemtrendsを使う場合（自分でmean -1sd, mean, mean + 1sdのレベルを予め指定する必要がある）
m_minus_SD = (mean(d$m) - sd(d$m)) # m = mean - 1SDのときにゼロになる変数を作成
m_Mean = mean(d$m)  # m = mean のときにゼロになる変数を作成
m_plus_SD = (mean(d$m) + sd(d$m)) # m = mean + 1SDのときにゼロになる変数を作成
m_list = list(m = c(m_minus_SD, m_plus_SD, m_Mean))

emmeans::emtrends(result_3, specs = "m", var = "x", at = m_list)

#単純傾斜の比較
emmeans::emtrends(result_3, pairwise ~ m, var = "x", at = m_list) 

```
